{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Result Visualization\n",
                "\n",
                "This notebook analyzes and visualizes training results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import roc_curve, precision_recall_curve, auc"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.1 Load Model and Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.utils import load_config, get_device\n",
                "from src.data import DatasetLoader, get_link_split\n",
                "from src.models import HeteroGCN, LinkPredictor\n",
                "\n",
                "config = load_config('../configs/config_base.yaml')\n",
                "device = get_device('cpu')\n",
                "\n",
                "# Load data\n",
                "loader = DatasetLoader('../data/processed')\n",
                "data = loader.load('microglial_cell_pinnacle')\n",
                "\n",
                "_, _, test_data = get_link_split(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "num_nodes = {nt: data[nt].num_nodes for nt in data.node_types}\n",
                "input_channels = {nt: data[nt].x.shape[1] for nt in data.node_types}\n",
                "\n",
                "model = HeteroGCN(\n",
                "    data.metadata(),\n",
                "    hidden_channels=config['model']['hidden_channels'],\n",
                "    num_layers=config['model']['num_layers'],\n",
                "    num_nodes_dict=num_nodes,\n",
                "    input_channels_dict=input_channels\n",
                ").to(device)\n",
                "\n",
                "predictor = LinkPredictor().to(device)\n",
                "\n",
                "# Load checkpoint\n",
                "ckpt = torch.load('../results/checkpoints/best_model.pth', weights_only=False)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "predictor.load_state_dict(ckpt['predictor_state_dict'])\n",
                "print(f\"Loaded checkpoint from epoch {ckpt['epoch']}\")\n",
                "print(f\"Saved metrics: {ckpt['metrics']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Generate Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "predictor.eval()\n",
                "\n",
                "test_data = test_data.to(device)\n",
                "target = ('drug', 'indication', 'disease')\n",
                "\n",
                "with torch.no_grad():\n",
                "    x_dict = {nt: test_data[nt].x for nt in test_data.node_types}\n",
                "    z_dict = model(x_dict, test_data.edge_index_dict)\n",
                "    \n",
                "    edge_idx = test_data[target].edge_label_index\n",
                "    edge_label = test_data[target].edge_label.numpy()\n",
                "    \n",
                "    scores = predictor(z_dict['drug'], z_dict['disease'], edge_idx)\n",
                "    probs = torch.sigmoid(scores).numpy()\n",
                "\n",
                "print(f'Test samples: {len(edge_label)}')\n",
                "print(f'Positive: {sum(edge_label)}, Negative: {sum(edge_label == 0)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 ROC Curve"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fpr, tpr, _ = roc_curve(edge_label, probs)\n",
                "roc_auc = auc(fpr, tpr)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
                "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.05])\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('Receiver Operating Characteristic (ROC)')\n",
                "plt.legend(loc='lower right')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.4 Precision-Recall Curve"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "precision, recall, _ = precision_recall_curve(edge_label, probs)\n",
                "pr_auc = auc(recall, precision)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(recall, precision, color='green', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
                "plt.axhline(y=sum(edge_label)/len(edge_label), color='navy', linestyle='--', label='Random')\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.05])\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "plt.title('Precision-Recall Curve')\n",
                "plt.legend(loc='upper right')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.5 Score Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_scores = probs[edge_label == 1]\n",
                "neg_scores = probs[edge_label == 0]\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.hist(neg_scores, bins=50, alpha=0.7, label='Negative', color='red')\n",
                "plt.hist(pos_scores, bins=50, alpha=0.7, label='Positive', color='blue')\n",
                "plt.xlabel('Prediction Score')\n",
                "plt.ylabel('Frequency')\n",
                "plt.title('Score Distribution')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.6 Top Predictions Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get node names\n",
                "node_mapping = pd.read_csv('../data/processed/node_mapping.csv')\n",
                "\n",
                "drugs = node_mapping[node_mapping['node_type'] == 'drug'].sort_values('node_idx')\n",
                "diseases = node_mapping[node_mapping['node_type'] == 'disease'].sort_values('node_idx')\n",
                "\n",
                "drug_names = drugs['node_name'].tolist()\n",
                "disease_names = diseases['node_name'].tolist()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top predictions\n",
                "drug_ids = edge_idx[0].numpy()\n",
                "disease_ids = edge_idx[1].numpy()\n",
                "\n",
                "results = pd.DataFrame({\n",
                "    'drug_idx': drug_ids,\n",
                "    'disease_idx': disease_ids,\n",
                "    'score': probs,\n",
                "    'label': edge_label\n",
                "})\n",
                "\n",
                "# Add names\n",
                "results['drug_name'] = results['drug_idx'].apply(lambda x: drug_names[x] if x < len(drug_names) else 'Unknown')\n",
                "results['disease_name'] = results['disease_idx'].apply(lambda x: disease_names[x] if x < len(disease_names) else 'Unknown')\n",
                "\n",
                "# Top 10 predictions (unknown)\n",
                "unknown = results[results['label'] == 0].nlargest(10, 'score')\n",
                "print('Top 10 Novel Predictions:')\n",
                "print(unknown[['drug_name', 'disease_name', 'score']])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.7 Comparison: Cell-Specific vs General"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare results from different configs\n",
                "results_comparison = {\n",
                "    'Model': ['Cell-Specific (PINNACLE)', 'General (Baseline)'],\n",
                "    'AUROC': [0.979, 0.975],  # Replace with actual values\n",
                "    'AUPRC': [0.983, 0.980],\n",
                "    'MRR': [0.45, 0.42]\n",
                "}\n",
                "\n",
                "comparison_df = pd.DataFrame(results_comparison)\n",
                "print(comparison_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bar plot comparison\n",
                "metrics = ['AUROC', 'AUPRC', 'MRR']\n",
                "x = np.arange(len(metrics))\n",
                "width = 0.35\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.bar(x - width/2, comparison_df.iloc[0][metrics], width, label='Cell-Specific')\n",
                "ax.bar(x + width/2, comparison_df.iloc[1][metrics], width, label='General')\n",
                "\n",
                "ax.set_ylabel('Score')\n",
                "ax.set_title('Cell-Specific vs General Model Performance')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(metrics)\n",
                "ax.legend()\n",
                "ax.set_ylim([0, 1])\n",
                "ax.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}